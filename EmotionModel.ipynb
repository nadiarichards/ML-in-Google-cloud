{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadiarichards/ML-in-Google-cloud/blob/main/EmotionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv20nr3l1BP0",
        "outputId": "60af1735-b963-4a80-afa3-9f7d71fd6dc7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx2_qmajm055",
        "outputId": "8f0304b6-9935-4bd2-9dba-eacbee4b4410"
      },
      "source": [
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa-uvP6DnmNN",
        "outputId": "cae06516-ac65-4bf7-bb38-74b0fd728e77"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRzuiVKhWBtK",
        "outputId": "1bc66b15-dab6-4c82-9bdb-df222bba49a5"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33MWsxWnorY",
        "outputId": "ad6b70f0-d04c-4472-f737-810feba2d3b2"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "# from keras.emotion_models import Sequential\n",
        "# from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# /content/gdrive/My Drive/Machine Learning Images Folder/train_test_images/train\n",
        "\n",
        "train_dir = \"/content/drive/My Drive/Machine Learning Images Folder/train_test_images/train\"\n",
        "val_dir = \"/content/drive/My Drive/Machine Learning Images Folder/train_test_images/test\"\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))\n",
        "# emotion_model.load_weights('emotion_model.h5')\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=28709 // 64,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=7178 // 64)\n",
        "emotion_model.save('/content/drive/My Drive/Machine Learning Images Folder/emotion_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28711 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "448/448 [==============================] - 8425s 19s/step - loss: 1.8295 - accuracy: 0.2432 - val_loss: 1.7000 - val_accuracy: 0.3598\n",
            "Epoch 2/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.6482 - accuracy: 0.3591 - val_loss: 1.5307 - val_accuracy: 0.4213\n",
            "Epoch 3/50\n",
            "448/448 [==============================] - 64s 144ms/step - loss: 1.5374 - accuracy: 0.4087 - val_loss: 1.4601 - val_accuracy: 0.4460\n",
            "Epoch 4/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.4607 - accuracy: 0.4434 - val_loss: 1.4146 - val_accuracy: 0.4605\n",
            "Epoch 5/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.4113 - accuracy: 0.4624 - val_loss: 1.3799 - val_accuracy: 0.4752\n",
            "Epoch 6/50\n",
            "448/448 [==============================] - 63s 142ms/step - loss: 1.3603 - accuracy: 0.4807 - val_loss: 1.3243 - val_accuracy: 0.4975\n",
            "Epoch 7/50\n",
            "448/448 [==============================] - 64s 142ms/step - loss: 1.3184 - accuracy: 0.4996 - val_loss: 1.2845 - val_accuracy: 0.5113\n",
            "Epoch 8/50\n",
            "448/448 [==============================] - 64s 142ms/step - loss: 1.2733 - accuracy: 0.5208 - val_loss: 1.2598 - val_accuracy: 0.5198\n",
            "Epoch 9/50\n",
            "448/448 [==============================] - 66s 146ms/step - loss: 1.2384 - accuracy: 0.5347 - val_loss: 1.2532 - val_accuracy: 0.5173\n",
            "Epoch 10/50\n",
            "448/448 [==============================] - 65s 144ms/step - loss: 1.2189 - accuracy: 0.5418 - val_loss: 1.2060 - val_accuracy: 0.5423\n",
            "Epoch 11/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.1630 - accuracy: 0.5641 - val_loss: 1.1865 - val_accuracy: 0.5476\n",
            "Epoch 12/50\n",
            "448/448 [==============================] - 63s 141ms/step - loss: 1.1366 - accuracy: 0.5759 - val_loss: 1.1709 - val_accuracy: 0.5551\n",
            "Epoch 13/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.1122 - accuracy: 0.5897 - val_loss: 1.1570 - val_accuracy: 0.5611\n",
            "Epoch 14/50\n",
            "448/448 [==============================] - 65s 145ms/step - loss: 1.0853 - accuracy: 0.5976 - val_loss: 1.1399 - val_accuracy: 0.5647\n",
            "Epoch 15/50\n",
            "448/448 [==============================] - 64s 143ms/step - loss: 1.0656 - accuracy: 0.6051 - val_loss: 1.1382 - val_accuracy: 0.5699\n",
            "Epoch 16/50\n",
            "448/448 [==============================] - 65s 145ms/step - loss: 1.0340 - accuracy: 0.6139 - val_loss: 1.1247 - val_accuracy: 0.5762\n",
            "Epoch 17/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 1.0230 - accuracy: 0.6181 - val_loss: 1.1213 - val_accuracy: 0.5766\n",
            "Epoch 18/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 1.0008 - accuracy: 0.6300 - val_loss: 1.1100 - val_accuracy: 0.5845\n",
            "Epoch 19/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.9680 - accuracy: 0.6435 - val_loss: 1.1076 - val_accuracy: 0.5824\n",
            "Epoch 20/50\n",
            "448/448 [==============================] - 65s 146ms/step - loss: 0.9456 - accuracy: 0.6481 - val_loss: 1.1008 - val_accuracy: 0.5840\n",
            "Epoch 21/50\n",
            "448/448 [==============================] - 67s 149ms/step - loss: 0.9283 - accuracy: 0.6591 - val_loss: 1.0937 - val_accuracy: 0.5963\n",
            "Epoch 22/50\n",
            "448/448 [==============================] - 67s 150ms/step - loss: 0.9023 - accuracy: 0.6702 - val_loss: 1.1037 - val_accuracy: 0.5910\n",
            "Epoch 23/50\n",
            "448/448 [==============================] - 67s 150ms/step - loss: 0.8813 - accuracy: 0.6788 - val_loss: 1.0929 - val_accuracy: 0.5935\n",
            "Epoch 24/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.8623 - accuracy: 0.6894 - val_loss: 1.0842 - val_accuracy: 0.6010\n",
            "Epoch 25/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.8234 - accuracy: 0.6981 - val_loss: 1.0786 - val_accuracy: 0.6024\n",
            "Epoch 26/50\n",
            "448/448 [==============================] - 65s 146ms/step - loss: 0.8173 - accuracy: 0.6973 - val_loss: 1.0802 - val_accuracy: 0.6052\n",
            "Epoch 27/50\n",
            "448/448 [==============================] - 65s 144ms/step - loss: 0.7896 - accuracy: 0.7090 - val_loss: 1.0745 - val_accuracy: 0.6048\n",
            "Epoch 28/50\n",
            "448/448 [==============================] - 67s 150ms/step - loss: 0.7819 - accuracy: 0.7158 - val_loss: 1.0954 - val_accuracy: 0.6010\n",
            "Epoch 29/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.7375 - accuracy: 0.7309 - val_loss: 1.0706 - val_accuracy: 0.6110\n",
            "Epoch 30/50\n",
            "448/448 [==============================] - 65s 145ms/step - loss: 0.7239 - accuracy: 0.7367 - val_loss: 1.0824 - val_accuracy: 0.6097\n",
            "Epoch 31/50\n",
            "448/448 [==============================] - 66s 146ms/step - loss: 0.6986 - accuracy: 0.7432 - val_loss: 1.0781 - val_accuracy: 0.6116\n",
            "Epoch 32/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.6755 - accuracy: 0.7556 - val_loss: 1.0810 - val_accuracy: 0.6131\n",
            "Epoch 33/50\n",
            "448/448 [==============================] - 67s 149ms/step - loss: 0.6667 - accuracy: 0.7648 - val_loss: 1.0977 - val_accuracy: 0.6152\n",
            "Epoch 34/50\n",
            "448/448 [==============================] - 66s 146ms/step - loss: 0.6463 - accuracy: 0.7648 - val_loss: 1.0863 - val_accuracy: 0.6133\n",
            "Epoch 35/50\n",
            "448/448 [==============================] - 65s 144ms/step - loss: 0.6186 - accuracy: 0.7780 - val_loss: 1.1038 - val_accuracy: 0.6187\n",
            "Epoch 36/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.6026 - accuracy: 0.7833 - val_loss: 1.1003 - val_accuracy: 0.6148\n",
            "Epoch 37/50\n",
            "448/448 [==============================] - 68s 152ms/step - loss: 0.5865 - accuracy: 0.7866 - val_loss: 1.1187 - val_accuracy: 0.6225\n",
            "Epoch 38/50\n",
            "448/448 [==============================] - 67s 150ms/step - loss: 0.5594 - accuracy: 0.8003 - val_loss: 1.1110 - val_accuracy: 0.6205\n",
            "Epoch 39/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.5446 - accuracy: 0.8039 - val_loss: 1.1166 - val_accuracy: 0.6203\n",
            "Epoch 40/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.5288 - accuracy: 0.8089 - val_loss: 1.1374 - val_accuracy: 0.6180\n",
            "Epoch 41/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.5108 - accuracy: 0.8172 - val_loss: 1.1391 - val_accuracy: 0.6184\n",
            "Epoch 42/50\n",
            "448/448 [==============================] - 68s 152ms/step - loss: 0.4990 - accuracy: 0.8213 - val_loss: 1.1558 - val_accuracy: 0.6218\n",
            "Epoch 43/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.4751 - accuracy: 0.8273 - val_loss: 1.1650 - val_accuracy: 0.6214\n",
            "Epoch 44/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.4572 - accuracy: 0.8361 - val_loss: 1.1491 - val_accuracy: 0.6186\n",
            "Epoch 45/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.4465 - accuracy: 0.8393 - val_loss: 1.1554 - val_accuracy: 0.6237\n",
            "Epoch 46/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.4369 - accuracy: 0.8440 - val_loss: 1.1776 - val_accuracy: 0.6242\n",
            "Epoch 47/50\n",
            "448/448 [==============================] - 67s 150ms/step - loss: 0.4130 - accuracy: 0.8547 - val_loss: 1.1850 - val_accuracy: 0.6191\n",
            "Epoch 48/50\n",
            "448/448 [==============================] - 66s 148ms/step - loss: 0.4154 - accuracy: 0.8491 - val_loss: 1.1843 - val_accuracy: 0.6267\n",
            "Epoch 49/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.3985 - accuracy: 0.8575 - val_loss: 1.1914 - val_accuracy: 0.6267\n",
            "Epoch 50/50\n",
            "448/448 [==============================] - 66s 147ms/step - loss: 0.3829 - accuracy: 0.8625 - val_loss: 1.2071 - val_accuracy: 0.6201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXZdUtKxCX0J"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "# from keras.emotion_models import Sequential\n",
        "# from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "-zflj8zbBKjs",
        "outputId": "8be58457-f7ca-481c-f634-1c06fcb1813a"
      },
      "source": [
        "# from tensorflow.keras.models import load_weights\n",
        "emotion_model = load_weights(\"/content/drive/My Drive/Machine Learning Images Folder/emotion_model.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fe1e97e358c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from tensorflow.keras.models import load_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Machine Learning Images Folder/emotion_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IWc4ub20DUP"
      },
      "source": [
        "# start the webcam feed\n",
        "emotion_model.load_weights('emotion_model.h5')\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    bounding_box = cv2.CascadeClassifier('/home/shivam/.local/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray_frame)\n",
        "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibU5guxNyF_Q"
      },
      "source": [
        "# emotion_model.load_weights('emotion_model.h5')\n",
        "model_save_name = 'emotions.h5'\n",
        "path = F\"/content/gdrive/My Drive/emotions.h5\" \n",
        "emotion_model.save_weights('emotions.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLWJmQZ08dQ"
      },
      "source": [
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g7OBeYu1REX",
        "outputId": "f9058d65-8376-4a9c-b623-aebad1da2415"
      },
      "source": [
        "model_json = emotion_model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "emotion_model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RESBBWhk0-5_"
      },
      "source": [
        "from keras.models import model_from_yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjsIh46Q1GbW",
        "outputId": "394fdc83-c277-4c0e-b2bf-560c33063fa7"
      },
      "source": [
        "model_yaml = emotion_model.to_yaml()\n",
        "with open(\"emotion_model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "emotion_model.save_weights(\"emotion_model_yaml.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "1rulSPI6S2ls",
        "outputId": "2dc48968-d796-4ef3-b208-19177a241761"
      },
      "source": [
        "emotion_model.load_weights('emotion_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-88dc6df51e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memotion_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emotion_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'emotion_model' is not defined"
          ]
        }
      ]
    }
  ]
}